<!doctype html><html lang="zh-tw">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="在〈分類與感知器〉中談到感知器的分類原理，是透過直線的法向量旋轉，直到法向量指向的那邊都是同一類，一個有趣的事實是，如果你將原始來源資料，代入求得的直線方程式：
# 都是大於 0
print(coef...">

    <meta property="og:locale" content="zh_TW">
    <meta property="og:title" content="分類與邏輯迴歸">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://openhome.cc/Gossip/DCHardWay/LogisticRegression.html">
    <meta property="og:image" content="https://openhome.cc/Gossip/images/caterpillar_small.jpg">
    <meta property="og:site_name" content="OPENHOME.CC">
    <meta property="og:description" content="在〈分類與感知器〉中談到感知器的分類原理，是透過直線的法向量旋轉，直到法向量指向的那邊都是同一類，一個有趣的事實是，如果你將原始來源資料，代入求得的直線方程式：
# 都是大於 0
print(coef...">

    <title>分類與邏輯迴歸</title>

    <link rel="stylesheet" href="../css/pure-0.6.0/pure-min.css">

    <!--[if lte IE 8]>
        <link rel="stylesheet" href="../css/layouts/side-menu-old-ie.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="../css/layouts/side-menu.css">
    <!--<![endif]-->
  

     <link rel="stylesheet" href="../css/caterpillar.css">
     <script async src="../google-code-prettify/run_prettify.js"></script>
     <!-- 網頁層級廣告 --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle =window.adsbygoogle || []).push({google_ad_client: "ca-pub-9750319131714390",enable_page_level_ads: true });</script></head>
<body>

<div id="layout">
    <!-- Menu toggle -->
    <a href="LogisticRegression.html#menu" id="menuLink" class="menu-link">
        <!-- Hamburger icon -->
        <span></span>
    </a>
    <div id="menu">
        <div class="pure-menu">
            <a class="pure-menu-heading" href="index.html">回 Python 資料科學</a>
            <ul class="pure-menu-list">
                <li class="pure-menu-item"><br><div class="social" style="text-align: center;"><a href="http://twitter.com/caterpillar"><img title="Twitter" alt="Twitter" src="../images/twitter.png"></a> <a href="http://www.facebook.com/openhome.cc"><img title="Facebook" alt="Facebook" src="../images/facebook.png"></a></div><br> <div id="search box"><script>(function() {var cx = 'partner-pub-9750319131714390:3926766884';var gcse = document.createElement('script');gcse.type = 'text/javascript';gcse.async = true;gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//www.google.com/cse/cse.js?cx=' + cx;var s = document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gcse, s);})();</script><gcse:searchbox-only></gcse:searchbox-only></div><br><div class="ad" style="text-align: center;"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 2015 新版型 160 x 600 廣告 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:160px;height:600px"
     data-ad-client="ca-pub-9750319131714390"
     data-ad-slot="3747048883"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div></li>
            </ul>
        </div>
    </div>

    <main id="main">
        <header class="header">
            <h1>分類與邏輯迴歸</h1>
        </header>

        <article class="content">
            <br><div class="ad-3" style="text-align: center;"><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- 2015 新版型回應式廣告 --><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9750319131714390" data-ad-slot="7104125683" data-ad-format="auto"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div> 
            
            <p>在〈<a href="Perceptron.html">分類與感知器</a>〉中談到感知器的分類原理，是透過直線的法向量旋轉，直到法向量指向的那邊都是同一類，一個有趣的事實是，如果你將原始來源資料，代入求得的直線方程式：</p>
<pre class="prettyprint"><code lang="python"># 都是大於 0
print(coef[0] * data[normal_weight, 0] + coef[1] * data[normal_weight, 1])
# 都是小於 0
print(coef[0] * data[overweight, 0] + coef[1] * data[overweight, 1])
</code></pre>
<p>你會發現，其中一類的結果都是大於 0（正常），另一類都是小於 0（胖），這並不是偶然，還記得〈<a href="Perceptron.html">分類與感知器</a>〉中談到的識別函式 f<sub>w</sub>(x) 嗎？識別函式的輸出，就是根據 <strong>W</strong> . <strong>X</strong> 是大於等於 0 或小於 0。</p>
<p>那麼若能找出一條線的程式，讓原始來源資料代入後，可以是大於 0 或小於 0，不就也可以進行分類？是的！從這個觀點出發，最後找出來可以分類的方式，稱為邏輯迴歸（Logistic regression），其名稱中有「迴歸」字眼，是因為在公式推導過程中，有用到迴歸的概念。</p>
<p>sklearn 提供了 <code>sklearn.linear_model.LogisticRegression</code>，可使用邏輯迴歸進行分類，單就線性可分來說，只要將〈<a href="Perceptron.html">分類與感知器</a>〉中的範例程式，從 <code>Perceptron</code> 改為 <code>LogisticRegression</code> 就可以了，例如：</p>
<pre class="prettyprint"><code lang="python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

from sklearn.linear_model import LogisticRegression

data = np.loadtxt('height_waist.csv', delimiter=',')

height_waist = data[:,0:2]
label = data[:,2]

normal_weight = label == 1
overweight = label == -1

# 使用 LogisticRegression 
lg_reg = LogisticRegression()
# 提供資料與標記
lg_reg.fit(height_waist, label)
# 取得權重向量
coef = lg_reg.coef_[0] 
# 截距
intercept = lg_reg.intercept_

plt.xlabel('height') 
plt.ylabel('waist')
plt.gca().set_aspect(1)
plt.scatter(data[normal_weight, 0], data[normal_weight, 1], marker = 'o')
plt.scatter(data[overweight, 0], data[overweight, 1], marker = 'x')

height = height_waist[:,0]

h = np.arange(np.min(height), np.max(height))
w = -coef[0] / coef[1] * h - intercept
plt.plot(h, w, linestyle='dashed')

plt.show()
</code></pre>
<p>求得的方程式，基本上與〈<a href="Perceptron.html">分類與感知器</a>〉中的範例非常接近：</p>
<p><div class="pure-g"><div class="pure-u-1"><img class="pure-img-responsive" src="images/LogisticRegression-1.JPG" alt="分類與邏輯迴歸"  /></div></div></p>
<p>對於非線性可分，可以透過邏輯迴歸來分類，例如，若〈<a href="Perceptron.html">分類與感知器</a>〉中對圖片的標記，每張照片其實還有體重的資訊，若身高、體重與標記存為 <a href="https://openhome.cc/Gossip/DCHardWay/height_weight.csv">height_weight.csv</a>：</p>
<pre class="prettyprint"><code lang="python">155,128,0
183,134,0
181,60,1
161,106,0
144,125,0
181,93,1
...略
</code></pre>
<p>如果根據這些資料來畫圖：</p>
<pre class="prettyprint"><code lang="python">import numpy as np
import matplotlib.pyplot as plt

data = np.loadtxt('height_weight.csv', delimiter=',')

height_weight = data[:,0:2]
label = data[:,2]

plt.xlabel('height') 
plt.ylabel('weight')

normal_weight = label == 1
overweight = label == 0

plt.scatter(data[normal_weight, 0], data[normal_weight, 1], marker = 'o')
plt.scatter(data[overweight, 0], data[overweight, 1], marker = 'x')

plt.show()
</code></pre>
<p>可以畫出以下的圖形，可以看出來不是線性趨勢：</p>
<p><div class="pure-g"><div class="pure-u-1"><img class="pure-img-responsive" src="images/LogisticRegression-2.JPG" alt="分類與邏輯迴歸"  /></div></div></p>
<p>在〈<a href="MultipleRegression2.html">多元線性迴歸（二）</a>〉中看過，結合 <code>PolynomialFeatures</code> 的話，<code>LinearRegression</code> 就可以用來達到多項式迴歸的目的，類似地，<code>PolynomialFeatures</code> 也可以結合 <code>LogisticRegression</code> 使用，例如：</p>
<pre class="prettyprint"><code lang="python">poly = PolynomialFeatures()                 # 二次多項式
feature = poly.fit_transform(height_weight) # 特徵值

lg_reg = LogisticRegression()  # 邏輯迴歸
lg_reg.fit(feature, label)
</code></pre>
<p>若只是要預測，可以透過 <code>predict</code> 方法，記得要先用 <code>fit_transform</code> 轉換：</p>
<pre class="prettyprint"><code lang="python"># 顯示 [1. 0.]
print(
    lg_reg.predict(
        poly.fit_transform([[178, 60], [183, 100]])
    )
)
</code></pre>
<p>如果要畫圖呢？在〈<a href="MultipleRegression2.html">多元線性迴歸（二）</a>〉中看過，二次的多項式特徵是 （1, x<sub>1</sub>, x<sub>2</sub>, x<sub>1</sub><sup>2</sup>, x<sub>1</sub> * x<sub>2</sub>, x<sub>2</sub<sup>2</sup>），因此，可以透過 <code>LogisticRegression</code> 的 <code>coef_</code> 取得係數後，結合橫軸值（也就是 x<sub>1</sub> 的值），整理之後，會是個 x<sub>2</sub> 的二次多項式，這時透過 <code>numpy</code> 的 <code>roots</code> 求平方根就可以了：</p>
<pre class="prettyprint"><code lang="python">import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures

data = np.loadtxt('height_weight.csv', delimiter=',')

# height weight
height_weight = data[:,0:2]
label = data[:,2]

poly = PolynomialFeatures()                 # 二次多項式
feature = poly.fit_transform(height_weight) # 特徵值

lg_reg = LogisticRegression()  # 邏輯迴歸
lg_reg.fit(feature, label)
coef = lg_reg.coef_[0]

plt.xlabel('height') 
plt.ylabel('weight')

normal_weight = label == 1
overweight = label == 0

plt.scatter(data[normal_weight, 0], data[normal_weight, 1], marker = 'o')
plt.scatter(data[overweight, 0], data[overweight, 1], marker = 'x')

height = height_weight[:,0]
h = np.arange(np.min(height), np.max(height)) 

ycoef0 = [coef[5]] * h.size
ycoef1 = coef[2] + coef[4] * h
ycoef2 = coef[0] + coef[1] * h + coef[3] * (h ** 2)

ycoef = np.dstack((ycoef0, ycoef1, ycoef2))[0]
y = np.apply_along_axis(np.roots, 1, ycoef) # 解平方根
w = y[:,1] # 只需要正值部份

plt.plot(h, w, linestyle='dashed')

plt.show()
</code></pre>
<p>畫出來的圖會是：</p>
<p><div class="pure-g"><div class="pure-u-1"><img class="pure-img-responsive" src="images/LogisticRegression-3.JPG" alt="分類與邏輯迴歸"  /></div></div></p>

            
           <br><br><div class="ad336-280" style="text-align: center;"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- 2015 新版型廣告 336 x 280 --><ins class="adsbygoogle" style="display:inline-block;width:336px;height:280px" data-ad-client="ca-pub-9750319131714390" data-ad-slot="9976409681"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div><br><div class="recommend" style="text-align: center;"><hr><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- 自動大小回應相符內容 --><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9750319131714390" data-ad-slot="4953478487" data-ad-format="autorelaxed"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div>
        </article>
    </main>
</div></body>
</html>
<script src="../js/ui.js"></script>
<div class="analytics"><script async src="https://www.googletagmanager.com/gtag/js?id=G-QVQQYFSC8J"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-QVQQYFSC8J');</script></div>
